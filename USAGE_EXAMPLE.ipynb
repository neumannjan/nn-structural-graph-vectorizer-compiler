{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def78307-c3cb-40b0-8a31-5cb7224daf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5812a-31f3-4f49-bee7-7edb3c52b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import compute_graph_vectorize.engines.torch as torch_engine\n",
    "from compute_graph_vectorize.engines.torch.settings import TorchModuleSettings\n",
    "from compute_graph_vectorize.sources.base import Network\n",
    "from compute_graph_vectorize.sources.builders import from_neuralogic\n",
    "from compute_graph_vectorize.sources.neuralogic_settings import NeuralogicSettings\n",
    "from compute_graph_vectorize.vectorize.pipeline.pipeline import create_vectorized_network_compiler\n",
    "from compute_graph_vectorize.vectorize.settings import VectorizeSettings, OptimizeSingleUseGathersSettings\n",
    "from neuralogic.core import Aggregation, R, Template, Transformation, V\n",
    "from neuralogic.dataset import TensorDataset\n",
    "from neuralogic.dataset.tensor import Data\n",
    "from torch_geometric.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf772c5-b059-48b3-bfe1-54b8e39d0f61",
   "metadata": {},
   "source": [
    "### Prepare Dataset and Template (Standard NeuraLogic Stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adefe2f-a1ed-4c3c-a9c1-16e2b155a462",
   "metadata": {},
   "source": [
    "NeuraLogic backend settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edc8ed5-2c6f-40ce-8cce-04832a7ed61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_settings = NeuralogicSettings()\n",
    "\n",
    "# MANDATORY FOR THE VECTORIZER TO WORK PROPERLY !\n",
    "n_settings.iso_value_compression = False\n",
    "n_settings.chain_pruning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b3e868-775e-4ee9-b64a-9ebd7d3e102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_template(num_features: int, output_size: int, dim: int = 10):\n",
    "    template = Template()\n",
    "\n",
    "    # template += (R.atom_embed(V.X)[dim, num_features] <= R.node_feature(V.X)) | [Transformation.IDENTITY]\n",
    "    # template += R.atom_embed / 1 | [Transformation.IDENTITY]\n",
    "\n",
    "    template += (R.l1_embed(V.X) <= (R.node_feature(V.Y)[dim, num_features], R._edge(V.Y, V.X))) | [\n",
    "        Aggregation.SUM,\n",
    "        Transformation.IDENTITY,\n",
    "    ]\n",
    "    template += R.l1_embed / 1 | [Transformation.RELU]\n",
    "\n",
    "    template += (R.l2_embed(V.X) <= (R.l1_embed(V.Y)[dim, dim], R._edge(V.Y, V.X))) | [\n",
    "        Aggregation.SUM,\n",
    "        Transformation.IDENTITY,\n",
    "    ]\n",
    "    template += R.l2_embed / 1 | [Transformation.IDENTITY]\n",
    "\n",
    "    template += (R.predict[output_size, dim] <= R.l2_embed(V.X)) | [Aggregation.AVG, Transformation.IDENTITY]\n",
    "    template += R.predict / 0 | [Transformation.SIGMOID]\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bc596b-c306-4529-bf9b-cfaaa950feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_dataset = TUDataset(root=\"./datasets\", name=\"MUTAG\")\n",
    "\n",
    "num_node_features = pyg_dataset.num_node_features\n",
    "\n",
    "dataset = TensorDataset(\n",
    "    data=[Data.from_pyg(data)[0] for data in pyg_dataset],\n",
    "    number_of_classes=num_node_features,\n",
    ")\n",
    "\n",
    "template = build_template(num_features=num_node_features, output_size=1, dim=10)\n",
    "\n",
    "# build the dataset in neuralogic\n",
    "built_dataset = template.build(n_settings).build_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dcf829-67e2-4799-a20c-4114bfedf2b2",
   "metadata": {},
   "source": [
    "### Build a Network representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47324f3b-7758-4559-9408-f94e9525ff73",
   "metadata": {},
   "source": [
    "This is essentially an API that accesses the internal neural network of the NeuraLogic Java backend. Other implementations of this API can be provided to support backends other than NeuraLogic. An implementation for Python dictionaries also exists (in case you want to write out the whole neural network in Python).\n",
    "\n",
    "Please see the Network API documentation in `compute_graph_vectorize/sources/base.py`.\n",
    "\n",
    "The NeuraLogic implementation already performs basic vectorization (grouping into layers). It also ensures that the layers are topologically ordered, so that if layer2 has layer1 on input, then layer1 comes before layer2 in `network.layers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7cfcbd-1cd0-48c1-b4e6-247095a7c60d",
   "metadata": {},
   "source": [
    "Build the Network container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9348ef5-3a89-4ebc-9486-b15166c1a131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkImpl((node_feature__f: FactLayer -> (length: 3371)), (l1_embed__wr: WeightedRuleLayer -> (length: 7442)), (l1_embed__ag: AggregationLayer -> (length: 3371)), (l1_embed__a: AtomLayer -> (length: 3371)), (l2_embed__wr: WeightedRuleLayer -> (length: 7442)), (l2_embed__ag: AggregationLayer -> (length: 3371)), (l2_embed__a: AtomLayer -> (length: 3371)), (predict__r: RuleLayer -> (length: 3371)), (predict__ag: AggregationLayer -> (length: 188)), (predict__wa: WeightedAtomLayer -> (length: 188)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = built_dataset.samples[:] # you can filter out samples here\n",
    "\n",
    "network: Network = from_neuralogic(samples, n_settings)\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45692b-7dea-408e-a4fb-5ef7d021e699",
   "metadata": {},
   "source": [
    "Below are some examples of how the Network API can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8082e7b3-3f48-4ac3-8433-519a0481d9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerDefinitionsImpl(node_feature__f, l1_embed__wr, l1_embed__ag, l1_embed__a, l2_embed__wr, l2_embed__ag, l2_embed__a, predict__r, predict__ag, predict__wa)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e5fd777-18ac-4763-b7e9-b2fe3978d19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_LayerNeurons(length: 3371)\n",
      "_Neurons(length: 0)\n",
      "_Ordinals((node_feature__f, 0), (node_feature__f, 1), (node_feature__f, 2), (node_feature__f, 3), (node_feature__f, 4), ... (length: 3371))\n",
      "_LayerNeurons(length: 7442)\n",
      "_Neurons(length: 7442)\n",
      "_Ordinals((node_feature__f, 0), (node_feature__f, 1), (node_feature__f, 0), (node_feature__f, 2), (node_feature__f, 3), ... (length: 7442))\n",
      "_Neurons(length: 0)\n",
      "_Ordinals( (length: 0))\n"
     ]
    }
   ],
   "source": [
    "print(network['node_feature__f'])\n",
    "print(network['node_feature__f'].inputs)\n",
    "print(network['node_feature__f'].ordinals)\n",
    "print(network['l1_embed__wr'])\n",
    "print(network['l1_embed__wr'].inputs)\n",
    "print(network['l1_embed__wr'].inputs.ordinals)\n",
    "print(network['l1_embed__wr'].inputs.inputs)\n",
    "print(network['l1_embed__wr'].inputs.inputs.ordinals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe7e2f-0da9-40bc-a73f-921e84b984c6",
   "metadata": {},
   "source": [
    "### Compile The Network Into Vectorized/Optimized Equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3a38c-8f9e-47c2-bedb-b5d7294b931e",
   "metadata": {},
   "source": [
    "We will not be using the Network API directly; we will be using the Compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef4635-55cb-4b65-9040-dfcbc93c94cd",
   "metadata": {},
   "source": [
    "Vectorizer/compiler settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b495f4-b6eb-496c-94d2-cf8ec30f6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may try some of the different presets for the optimize_single_use_gathers optimization:\n",
    "# (Other optimizations can be left default)\n",
    "v_settings = VectorizeSettings(\n",
    "    #optimize_single_use_gathers=OptimizeSingleUseGathersSettings.preset(\"agg_true_unlimited\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89e226f5-380a-4a0a-a8c2-fb77fcc16da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Very verbose when True! Prints the whole network along every optimization step.\n",
    "debug_prints = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e1fd7-d538-46f8-bcd4-fad971ff98d7",
   "metadata": {},
   "source": [
    "Build the compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1400373-b3fa-40b5-b315-95025f996d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler = create_vectorized_network_compiler(\n",
    "    v_settings,\n",
    "    forward_pass_runner=torch_engine.torch_simple_forward_pass_runner,  # needed for ISO compression\n",
    "    debug_prints=debug_prints,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019eabe2-75a4-4077-8de5-ce53e2032990",
   "metadata": {},
   "source": [
    "Compile the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0066833-4535-416b-8034-ccfeb5bdf56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorizedOpSeqNetwork(\n",
       "  (fact_layers): {\n",
       "    (f_node_feature__f__0): FactLayer([1, 7, 1], [1, 7, 1], [1, 7, 1], ... (size: 7), count=7, shape=[7, 1])\n",
       "  }\n",
       "  (weights): {\n",
       "    (w_000): LearnableWeight(value=array((1, 10, 7)))\n",
       "    (w_001): LearnableWeight(value=array((1, 10, 10)))\n",
       "    (w_002): LearnableWeight(value=array((1, 1, 10)))\n",
       "  }\n",
       "  (batches): {\n",
       "    (0): OpSeqBatch(\n",
       "      (layers): {\n",
       "        (l_predict__wa): OperationSeq(\n",
       "          expected_count=188\n",
       "          (layer_refs): LayerRefs(<f|f_node_feature__f__0>)\n",
       "          (0): Linear(\n",
       "            (weight_ops): OperationSeq(\n",
       "              expected_count=None\n",
       "              (layer_refs): LayerRefs(<w|w_000>)\n",
       "            )\n",
       "          )\n",
       "          (1): GenericGather(ordinals=[0, 0, 0, ... (size: 75)])\n",
       "          (2): UnevenReduce(counts=[2, 3, 3, ... (size: 28)], total=75, reduce='sum')\n",
       "          (3): Transform(transform='relu')\n",
       "          (4): Linear(\n",
       "            (weight_ops): OperationSeq(\n",
       "              expected_count=None\n",
       "              (layer_refs): LayerRefs(<w|w_001>)\n",
       "            )\n",
       "          )\n",
       "          (5): GenericGather(ordinals=[0, 1, 0, ... (size: 556)])\n",
       "          (6): UnevenReduce(counts=[2, 2, 3, ... (size: 223)], total=556, reduce='sum')\n",
       "          (7): GenericGather(ordinals=[0, 1, 0, ... (size: 3371)])\n",
       "          (8): UnevenReduce(counts=[17, 13, 13, ... (size: 188)], total=3371, reduce='average')\n",
       "          (9): Linear(\n",
       "            (weight_ops): OperationSeq(\n",
       "              expected_count=None\n",
       "              (layer_refs): LayerRefs(<w|w_002>)\n",
       "            )\n",
       "          )\n",
       "          (10): Transform(transform='sigmoid')\n",
       "        )\n",
       "      }\n",
       "    )\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_network = compiler(network)\n",
    "vectorized_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb0929-42ef-4f6a-9201-3a8be5b8736f",
   "metadata": {},
   "source": [
    "We may look into some internals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863538e9-b3af-4588-a251-3af7f93dfa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_network.fact_layers['f_node_feature__f__0'].facts[0].value.flatten())\n",
    "print(vectorized_network.fact_layers['f_node_feature__f__0'].facts[1].value.flatten())\n",
    "print(vectorized_network.fact_layers['f_node_feature__f__0'].facts[2].value.flatten())\n",
    "print(vectorized_network.fact_layers['f_node_feature__f__0'].facts[3].value.flatten())\n",
    "print(vectorized_network.fact_layers['f_node_feature__f__0'].facts[4].value.flatten())\n",
    "print(vectorized_network.fact_layers['f_node_feature__f__0'].facts[5].value.flatten())\n",
    "print(vectorized_network.fact_layers['f_node_feature__f__0'].facts[6].value.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "499e8121-4e9f-42eb-9f78-7d0ba5a7ad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 2, 2, 2, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 4, 0, 0, 2, 5, 0, 0, 0, 0, 3, 3, 3, 0, 3, 1, 0, 2, 2, 0, 0, 6, 0, 0, 5, 0, 2, 1, 2, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 6, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_network.batches[0].layers['l_predict__wa'][1].ordinals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e781f2e-b22c-481a-8bec-bef85b8001a9",
   "metadata": {},
   "source": [
    "#### Optional Exercise\n",
    "\n",
    "Go back and try the different presets of the `optimize_single_use_gathers` optimization.\n",
    "You will see how it affects the different gather/reduce operations in the network above.\n",
    "\n",
    "You can also try changing its individual configuration options manually, as well as changing other configuration options as well, to see how it affects the resulting vectorized network.\n",
    "\n",
    "For example, you may see what happens when you disable some optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0cfd4-298f-47ed-bd5b-3635ac902a3c",
   "metadata": {},
   "source": [
    "### Train in PyTorch (on any PyTorch-supported hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227c06d-4904-4669-8248-f1fb2c56b0f2",
   "metadata": {},
   "source": [
    "PyTorch engine settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc15c0e-5e6d-469a-ba6b-0a56ddc18afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_settings = TorchModuleSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf030b70-46e7-4726-9cd5-561907477261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debug for easier understanding of exceptions during the forward pass\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75e8cbbd-e61c-49ca-9861-e6d15517e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true: each forward pass returns only the final output tensor.\n",
    "# If false: each forward pass returns a dict of layer name -> tensor.\n",
    "final_layer_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b5424-76d2-4864-8256-f5fb685f9c74",
   "metadata": {},
   "source": [
    "Build the PyTorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd9ee387-f2d5-451c-8b90-f69f99a06428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkModule(\n",
       "  (params_module): NetworkParams(\n",
       "    (params): ParameterDict(\n",
       "        (f_node_feature__f__0): Parameter containing: [torch.FloatTensor of size 7x7x1]\n",
       "        (w_000): Parameter containing: [torch.FloatTensor of size 1x10x7]\n",
       "        (w_001): Parameter containing: [torch.FloatTensor of size 1x10x10]\n",
       "        (w_002): Parameter containing: [torch.FloatTensor of size 1x1x10]\n",
       "    )\n",
       "  )\n",
       "  (batch_modules): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): LayerModule(\n",
       "        out_key: l_predict__wa,\n",
       "        expected_count: 188,\n",
       "        (the_modules): ModuleList(\n",
       "          (0): RetrieveRefModule(f_node_feature__f__0)\n",
       "          (1): LinearModule(\n",
       "            (retrieve_weights): Sequential(\n",
       "              (0): RetrieveRefModule(w_000)\n",
       "            )\n",
       "          )\n",
       "          (2): GenericGatherModule([0, 0, 0, ... (size: 75)])\n",
       "          (3): SegmentCSR(reduce=sum, count=28)\n",
       "          (4): ReLU()\n",
       "          (5): LinearModule(\n",
       "            (retrieve_weights): Sequential(\n",
       "              (0): RetrieveRefModule(w_001)\n",
       "            )\n",
       "          )\n",
       "          (6): GenericGatherModule([0, 1, 0, ... (size: 556)])\n",
       "          (7): SegmentCSR(reduce=sum, count=223)\n",
       "          (8): GenericGatherModule([0, 1, 0, ... (size: 3371)])\n",
       "          (9): SegmentCSR(reduce=mean, count=188)\n",
       "          (10): LinearModule(\n",
       "            (retrieve_weights): Sequential(\n",
       "              (0): RetrieveRefModule(w_002)\n",
       "            )\n",
       "          )\n",
       "          (11): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (1): RetrieveRefModule(l_predict__wa)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model = torch_engine.build_torch_model(\n",
    "        vectorized_network,\n",
    "        t_settings,\n",
    "        debug=debug,\n",
    "        final_layer_only=final_layer_only\n",
    ")\n",
    "torch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9006765-6cd8-4710-9abd-ea9152bf6a4e",
   "metadata": {},
   "source": [
    "Run a forward pass on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10afe7e-b908-477b-883d-092c2912f8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "output = torch_model(batch=0) # Note: Currently batching is not supported, but the data model is ready for it.\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97563413-de7c-4133-bc85-7a429ca15ae8",
   "metadata": {},
   "source": [
    "You can now run your typical PyTorch training pipeline on top of `torch_model`.\n",
    "\n",
    "If you are having problems on your hardware, try setting `t_settings.reduce_method = \"scatter\"` (before building the torch model).\n",
    "If you are still having problems on your hardware, it may be because gather/scatter operations are not supported on it in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d0119-072a-4dca-b3d4-9413d2ebaf7c",
   "metadata": {},
   "source": [
    "### Extract Weights Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "132c584d-ef0c-44bb-8f20-3ad4cc0fa119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'000': torch.Size([1, 10, 7]), '001': torch.Size([1, 10, 10]), '002': torch.Size([1, 1, 10])}\n"
     ]
    }
   ],
   "source": [
    "weights = torch_engine.extract_weights_from_torch_model(torch_model)\n",
    "print({k: v.shape for k, v in weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ec03d-00c2-4772-9eaf-bfff948114b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neuralogic-torch3]",
   "language": "python",
   "name": "conda-env-neuralogic-torch3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
